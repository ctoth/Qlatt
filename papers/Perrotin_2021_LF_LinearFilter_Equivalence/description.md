This paper demonstrates that computationally efficient linear-filter implementations (LFCALM and LFLM) of the LF glottal flow model produce perceptually equivalent results to the original LF model while being 10-100 times faster computationally. The authors establish a unified mathematical framework for all three models and validate perceptual equivalence through listening experiments showing that voice quality parameter Rd dominates perception while model differences are minimal. For speech synthesis systems like Klatt, these findings enable real-time glottal source generation with efficient digital filter implementations while maintaining perceptual naturalness and offering precise voice quality control through the single Rd parameter.
