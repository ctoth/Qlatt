# Pages 160-199 Notes

## Chapters/Sections Covered
- **2.5 Spectral Voice Model** (continued from previous chunk, pp. 161-170)
  - EpR Transformations (pp. 161-163)
  - 2.5.2 Modeling the Phase Envelope (pp. 164-168)
- **2.6 Conclusions** (pp. 169-170)
- **Chapter 3: Singing Synthesis by Performance Sampling** (pp. 171-199)
  - 3.1 Sampling the Sonic Space (pp. 172-178)
    - 3.1.1 A Performance based Sampling Synthesizer (p. 173)
    - 3.1.2 An additive vowel synthesizer (pp. 173-178)
  - 3.2 Performance Database (pp. 179-188)
    - 3.2.1 Defining the sonic space (pp. 179-180)
    - 3.2.2 Recording scripts (pp. 182-183)
    - 3.2.3 Recording session (p. 184)
    - 3.2.4 Database creation (pp. 184-188)
  - 3.3 Performer Model (pp. 189-190)
    - 3.3.1 Approaches to Performance Modeling (pp. 189-190)
    - 3.3.2 Singing Voice Performance Controls (pp. 190)
  - 3.4 Performance Trajectory Generation (pp. 191-198)
    - 3.4.1 Phonetic Sequence (pp. 192)
    - 3.4.2 Pitch and Dynamics Envelopes (pp. 194-198)
  - 3.5 Sound Rendering (pp. 199-200)
    - 3.5.1 Concatenating Samples (pp. 199)

## Key Findings

### EpR Voice Model
- The EpR spectral envelope is computed as the sum in dB of all three filters (source, resonances, residual)
- Parameters relate to perceptually relevant voice features:
  - Source filter gain/slope/slope-depth relates to brightness/high frequencies
  - Resonances relate to phonetics and personality
  - Residual envelope contains low-level nuances missing from smooth resonance model
- Residual envelope is anchored to formant frequencies for interpolation
- When formants are shifted, residual regions between formants are compressed or stretched accordingly

### Phase Envelope Modeling
- Strong correlation between harmonic amplitude envelope and phase alignment at voice pulse onsets
- Phase envelope can be approximated from amplitude derivative
- 19dB amplitude difference corresponds to pi radians phase shift
- Phase model parameters: o=3 (smoothing order), phi_M=-2 (phase offset), alpha=pi/19 (scaling factor)
- For frequencies above 8kHz, sinusoidal phase correction is added to avoid flat phase artifacts

### Chapter 2 Conclusions
- Voice utterances can be interpreted as time-varying frequency components
- NBVPM (Narrow Band Voice Pulse Model) and WBVPM (Wide Band Voice Pulse Model) introduced
- WBVPM provides independent control of each harmonic component
- MFPA method for approximating GCIs (Glottal Closure Instants)
- Subharmonics method for emulating growls and other expressive effects
- EpR amplitude spectral model uses source-filter decomposition with residual envelope
- Phase model predicts harmonic phases at voice pulse onsets without significantly altering timbre

### Singing Synthesis Concepts
- Sonic space = multidimensional collection of sounds producible by performer+instrument
- Space A = all possible sounds from instrument
- Space B = subset producible by specific performer
- Recorded samples = trajectories through this space
- Goal: parameterize space to enable interpolation and transformation

### Performance-Based Sampling Synthesizer Architecture
1. Performance Score (high-level input)
2. Performer Model (converts to mid-level actions)
3. Performance Trajectory Generator (creates parameter trajectories)
4. Sound Rendering (produces output by concatenating transformed samples)
5. Performance Database (stores samples and models)

### Additive Vowel Synthesizer
- Proof of concept using EpR model for singing vowels
- Inputs: MIDI (pitch, volume, duration, articulation) + formant frequencies (F1, F2)
- Vowel space stored in database with EpR models at different pitches/dynamics
- 3 vowels (/i/, /o/, /u/) used for triangular interpolation of any vowel target
- Blackman-Harris 92dB window used for additive synthesis (9 bins sufficient)

### Database Dimensions
- Subspace A: phonetic, tempo, loudness, pitch (actual samples)
- Subspace B: vibrato type (operatic, jazzy, etc.)
- Subspace C: articulation type (soft/sharp attack, legato/portamento transition)

### Spanish Phonetic Dictionary
- 33 Spanish allophones (SAMPA notation)
- 521 di-allophone combinations to record
- Covers 94% of all possible appearing combinations
- 28 different contexts per di-allophone (3 pitches x 2 loudness x 4 stationary + tempos)

### Recording Process
- Spanish: 3-4 hours per singer
- English: 5-6 hours (larger phoneme inventory)
- Uses automatic phoneme segmentation (ASR toolkit)
- Post-processing with MFCCs, amplitude envelope, zero-crossing rate
- DTW (Dynamic Time Warping) for sample alignment

## Equations Found

### EpR Spectral Envelope (Eq. 2.107)
$$EpR_{dB}(f) = Gain_{dB} + SlopeDepth_{dB}(e^{Slope \cdot f} - 1) + 20\log_{10}\left(\sum_{i=0}^{M} R_i(f)\right) + S_{residual_{dB}}(f)$$

Where:
- $Gain_{dB}$ = source filter gain
- $SlopeDepth_{dB}$ = source slope depth
- $Slope$ = source slope parameter
- $R_i(f)$ = resonance filter contribution at frequency f
- $S_{residual_{dB}}(f)$ = residual spectral envelope

### Phase Prediction from Amplitude Derivative (Eq. 2.108)
$$\hat{\phi}_h = \alpha \cdot 20\log_{10}\left(\frac{a_{h+1}}{a_h}\right) \quad \text{for } h = 0...H-2$$

Where:
- $\hat{\phi}_h$ = predicted phase for harmonic h
- $a_h$ = amplitude of harmonic h
- $\alpha$ = scaling factor (pi/19 is good choice)
- $H$ = number of harmonics

### Phase Smoothing (Eq. 2.109)
$$\hat{\phi}_{h,smoothed} = \phi_M + \frac{1}{o}\sum_{k=h-\frac{o-1}{2}}^{h+\frac{o-1}{2}} \hat{\phi}_{\max(0,\min(H-1,k))} \quad \text{for } h = 0...H-1$$

Where:
- $o$ = filter order (odd)
- $\phi_M$ = phase offset

### High-Frequency Phase Correction (Eq. 2.111)
$$\hat{\phi}'_{h,smoothed} = \phi'_M + \frac{1}{o}\sum_{k=h-\frac{o-1}{2}}^{h+\frac{o-1}{2}} \hat{\phi}'_{\max(0,\min(H'-1,k))} + \begin{cases} 0 & \text{if } h < D \\ \pi\sin\left(2\pi\frac{h-D}{E}\right) & \text{if } h \geq D \end{cases}$$

Where:
- $D$ = index of first harmonic above 8kHz
- $E$ = 35 (sinusoid rate)

### Vowel Quality Interpolation (Eq. 3.1)
$$VOWEL\ QUALITY \quad EpR_{target} = \frac{\sum_v \omega_v V_v}{\sum_v \omega_v}$$
$$PITCH \quad V_v = note_{v,n}(1 - intp_{v,n}) + intp_{v,n} \cdot note_{v,n+1}$$
$$DYNAMICS \quad note_{v,n} = dyn_{v,n,d}(1 - intp_{v,n,d}) + intp_{v,n,d} \cdot dyn_{v,n,d+1}$$

### Formant Triangle Interpolation
For point P inside triangle formed by vowels /i/, /o/, /u/:
$$A = /i/ + \frac{F1_P - F1_{/i/}}{F1_{/o/} - F1_{/i/}}(/o/ - /i/)$$
$$B = /u/ + \frac{F1_P - F1_{/u/}}{F1_{/o/} - F1_{/u/}}(/o/ - /u/)$$
$$P = A + \frac{F2_P - F2_A}{F2_B - F2_A}(B - A)$$

### Harmonic Frequency (Eq. 3.2)
$$f_h = (h+1)f_0 \quad \text{for } h \in 0...H-1$$

### Sample Sequence Cost Function (Eq. 3.3)
$$C(\bar{S}) = \sum_{i=0}^{n-1} C(\bar{S}(i))$$
$$C(S_i) = w_{pitch}C_{pitch}(S_i) + w_{dynamics}C_{dynamics}(S_i) + w_{duration}C_{duration}(S_i)$$
$$+ w_{pitch\_cont}C_{pitch\_cont}(S_i) + w_{dynamics\_cont}C_{dynamics\_cont}(S_i)$$

### Best Sequence Selection (Eq. 3.4)
$$C(\bar{S}_{best}) \leq C(\bar{S}_k) \quad \forall k \in \{0, m-1\}$$

## Parameters Found

| Name | Symbol | Units | Value/Range | Notes |
|------|--------|-------|-------------|-------|
| Phase scaling factor | alpha | - | pi/19 | 19dB difference = pi radians |
| Smoothing filter order | o | - | 3 (odd) | Running average for phase smoothing |
| Phase offset | phi_M | radians | -2 | Base phase offset value |
| High-freq sinusoid index | D | - | First harmonic > 8kHz | Start of sinusoidal phase correction |
| High-freq sinusoid rate | E | - | 35 | Controls sinusoid period |
| Analysis window | - | samples | 256 | For phase envelope analysis |
| Hop size | - | samples | 256 | For voice pulse alignment |
| Spanish allophones | - | count | 33 | SAMPA notation |
| Di-allophone combinations | - | count | 521 | To record for 94% coverage |
| Pitches per di-allophone | - | count | 3 | Per loudness level |
| Loudness levels | - | count | 2 | soft, normal |
| Pitch standard deviation | sigma | cents | 20 | For onset/ending notes |
| Note pitch valley | v1mean | cents | 25 | Mean valley depth |
| Duration clusters | d2 | seconds | 0.05 | Cluster threshold |
| Mean before onset | pMeanBeforeOnsetFirstNote | cents | 150 | Pitch before first note onset |
| Dev before onset | pDevBeforeOnsetFirstNote | cents | 30 | Std dev before first note onset |
| Mean at onset | pMeanOnsetFirstNote | cents | 50 | Pitch at note onset |
| Dev at onset | pDevOnsetFirstNote | cents | 30 | Std dev at note onset |
| Distance before onset | dBeforeOnsetFirstNote | seconds | 0.2 | Time before first note onset |
| Valley mean transition | vMeanNoteTransition | cents | 30 | Valley added to note transition |
| Valley dev transition | vDevNoteTransition | cents | 20 | Valley std dev |
| Note transition power | NoteTransitionPow | - | 0.7 | Shape control |
| Mean at ending | pMeanEndingNote | cents | 20 | Pitch at note ending offset |
| Dev at ending | pDevEndingNote | cents | 10 | Std dev at ending |
| Decay pitch ending | pDecayEndingNote | cents | 50 | Pitch decay at ending |
| Decay position ending | dDecayEndingNote | seconds | 0.5 | Position of decay |

## Rules/Algorithms

### Phase Envelope Computation Algorithm
1. Compute harmonic amplitudes at voice pulse onset
2. Calculate amplitude derivative between consecutive harmonics
3. Scale derivative by alpha (pi/19) to get phase values
4. Apply running average smoothing (order o=3)
5. Add phase offset phi_M
6. For harmonics above 8kHz, add sinusoidal correction

### Vowel Space Interpolation
1. Identify which triangle in formant map contains target point P
2. Compute weights A and B by interpolating along F1 axis
3. Compute final voice model P by interpolating between A and B along F2 axis
4. For given pitch and dynamics, interpolate between database entries
5. Apply weighted sum of three closest vowel models

### Database Creation Process
1. Manually cut audio files into sentences
2. Add phonetic transcription with tempo/pitch/loudness values
3. Perform automatic phoneme segmentation (ASR toolkit)
4. Apply di-allophone segmentation with rules based on phoneme type
5. Compute low-level descriptors (MFCCs, envelope derivative, zero-crossing)
6. Detect gaps, stops, and stable segments
7. Store in chunk-based binary format with folder tree structure
8. Perform sample analysis (VPM, EpR parameters, residual/transients)

### Sample Concatenation Smoothing
1. Compute differences at joint points for several voice features
2. Spread correction values around connection points
3. Apply corrections to pitch and voice model parameters
4. Use phase model to avoid phase discontinuities when possible

### Pitch Model Point Generation
1. For regular duration notes: generate 3 points (A, B, C)
2. For shorter notes: generate 2 points (A, B)
3. For very short notes: generate 1 point (A)
4. Section A-B: valley computed from normal distribution (mean=0, sigma_B=35 cents)
5. Section B-C: linear interpolation
6. If note is long, add more random points between B and C (every 200ms)
7. Apply portamento/scoop by shifting note onsets in pitch curve

## Figures of Interest
- **Fig 2.111 (p. 161):** EpR voice model analysis showing waveform, MFPA predicted onsets, source/filter resonances, residual envelope, and amplitude/phase spectra
- **Fig 2.112 (p. 162):** EpR voice model step-by-step block diagram showing harmonic envelope to voice source to vocal tract to residual processing chain
- **Fig 2.113 (p. 163):** Envelope interpolation vs resonance interpolation comparison - demonstrates that resonance model preserves formant identity during transformation while envelope interpolation flattens formants
- **Fig 2.114 (p. 164):** Vibrato transformation showing how EpR model handles pitch modulation while preserving spectral envelope
- **Fig 2.115 (p. 165):** Phase alignment computation from spectral envelope scaling and shifting
- **Fig 2.116 (p. 166):** Phase envelope from wide-band spectral amplitude derivative - shows waveform, magnitude spectrum, and phase spectrum aligned with envelope derivative
- **Fig 2.117 (p. 167):** Comparison of original audio and resynthesis with phase model (alpha=pi/11 vs alpha=pi/19) - demonstrates perceptual differences
- **Fig 3.1 (p. 172):** Instrument sonic space concept - nested spaces showing instrument possibilities, performer subset, and recorded samples as trajectories
- **Fig 3.2 (p. 173):** Performance-based sampling synthesizer block diagram
- **Fig 3.3 (p. 174):** Additive vowel synthesizer architecture with F1/F2 formant map
- **Fig 3.4 (p. 175):** Vowel synthesizer database showing Spanish /a/ vowel analysis
- **Fig 3.5 (p. 176):** Vowel quality formant map with triangular interpolation regions
- **Fig 3.7 (p. 180):** Proposed subspaces of singing voice sonic space (A: phonetic/tempo/loudness/pitch, B: vibrato styles, C: articulation types)
- **Fig 3.8 (p. 182):** Loudness and pitch variations inherent to phonetic articulations (/a/-/g/-/a/ transition)
- **Fig 3.10 (p. 184):** Recording studio facilities at IUA/UPF
- **Fig 3.11 (p. 185):** Singer database creation process flowchart
- **Fig 3.12 (p. 186):** Gaps, stops, and stable segment detection in /a/-/k/-/a/ transition
- **Fig 3.13 (p. 187):** Voice database creation authoring tool screenshot
- **Fig 3.14 (p. 188):** Voice database folder structure
- **Fig 3.15 (p. 188):** Voice database creation VST plug-in interface
- **Fig 3.16 (p. 191):** Performance trajectory example showing samples, phonetic sequence, pitch, dynamics, and voice model controls
- **Fig 3.17 (p. 193):** From performance score to performance trajectory - complete processing chain
- **Fig 3.18 (p. 195):** Pitch model based on interpolating points - shows different configurations for different note durations
- **Fig 3.19 (p. 197):** Portamento and scoop example - waveform and pitch curve for "the day I fall in love"
- **Fig 3.20 (p. 198):** Vibrato template with EpR control envelopes (gain, slope, slope depth, pitch)
- **Fig 3.21 (p. 198):** Harmonic frequency excursion during vibrato - demonstrates how harmonics follow EpR spectral envelope
- **Fig 3.22 (p. 199):** Note transition template with scaled pitch correction
- **Fig 3.23 (p. 200):** Matching performance trajectory by transforming samples
- **Fig 3.24 (p. 200):** Sample concatenation smoothing showing discontinuity correction

## Quotes Worth Preserving

> "Sampling has always been considered a way to capture and reproduce the sound of an instrument but in fact it should be better considered a way to model the sonic space produced by a performer with an instrument." (p. 172)

> "The abrupt closure of the vocal folds often produces a prominent excitation to the vocal tract, an impulse that has minimal phase characteristics." (p. 164)

> "Actually, one of the main difficulties of using a formant model to predict the phase envelope is that it is a discrete model that has to take binary decisions regarding the presence of formants." (p. 164)

> "We found that the clarity and sharpness varied from less to more between alpha=pi/7 and alpha=pi/21. We found that the feeling of clarity and sharpness changed gradually, being phi_M=-2 and phi_M=0.5 the offsets that produced the best and worse results respectively." (p. 167)

> "Using a phase harmonic model has several advantages. One of the most evident is data compression, since there is no need to store harmonic phases." (p. 168)

> "Singing voice is a clear example that the basic sampling approach is not adequate and that a parameterization of the sounds is required." (p. 172)

> "In singing voice is never static but constantly changing along time. That is an important characteristic to emulate; otherwise, synthesized utterances are perceived as rather robotic or artificial." (p. 196)

## Implementation Notes

### Phase Model Implementation
- Store only amplitude envelope, compute phase at synthesis time
- Use alpha=pi/19, o=3, phi_M=-2 as default parameters
- Apply sinusoidal correction above 8kHz (D = first harmonic > 8kHz, E=35)
- Zero-delay running average filter of 5 coefficients for smoothing
- Phase computed from synthesis harmonic amplitudes, not input harmonics

### Vowel Synthesizer Data Structures
- EpR parameters stored per vowel (/a/, /e/, /i/, /o/, /u/) per pitch per dynamics
- Database indexed by (F1, F2) coordinates for fast lookup
- Triangular tessellation of vowel space for interpolation
- Vibrato/sustain templates stored separately in subspace B

### Database Structure
- Folder tree: singerName/voice/stationary/sustain/normal/vowel/pitch/loudness
- Binary .dat files with chunk-based format
- Chunk types: DBSinger, DBVoice, ChunkArray, DBSustain, DBSustainPhU, DBSustainPhUPart
- Articulation data: DBArticulation, DBArticulationTargetPhU chunks
- Each folder accompanied by same-name .dat file

### Sample Selection Algorithm
- Cost function weights: w_pitch, w_dynamics, w_duration, w_pitch_cont, w_dynamics_cont
- Empirically determined weights
- Optimization limited to 2 succeeding notes for real-time performance
- Global optimization preferred but computationally expensive

### Portamento/Scoop Implementation
- Portamento: reaching note pitch before actual note onset
- Scoop: beginning note transition after actual note onset
- Shift note onset positions in pitch model, keep phonetic boundaries fixed
- Both affect pitch curve but not phonetic track alignment

### Audio Database Statistics (Table 2.7)
- Test database: 5 male speech, 3 female speech, 8 male singing, 7 female singing
- Singing styles: jazz, soul, dance, blues, pop, scat, Louis Armstrong characteristic
- Various expressive resources: vibrato, scoop, glissando, growl
- Audio files sampled at 44.1kHz, quantized to 16 bits
