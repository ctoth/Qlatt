This paper presents a hybrid model of speech that divides utterances into two perceptual units: acoustic nuclei (vowels and sonorants that carry speaker identity) and acoustic consonant clusters (consonants that are speaker-generic). The study demonstrates that 60-70% of an utterance can be replaced with another speaker's consonants using simple rule-based formant synthesis without degrading speaker identification, while listeners cannot detect that voices are hybrid. This work is critical for speech synthesis because it shows that consonants need not be speaker-specific, timing relationships are more important than spectral accuracy, and formant synthesis rules can generate perceptually generic consonants across speakers.