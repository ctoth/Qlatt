# Citations

## Reference List

[1] K. R. Scherer, "Vocal affect expression: A review and a model for future research," Psychological Bulletin, vol. 99, pp. 143–165, 1986.

[2] R. Banse and K. R. Scherer, "Acoustic Profiles in Vocal Emotion Expression," Journal of Personality and Social Psychology, vol. 70, no. 3, pp. 614–636, 1996.

[3] P. N. Juslin and P. Laukka, "Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion," Emotion, vol. 1, pp. 381–412, 2001.

[4] S. Yildirim, M. Bulut, C. Lee, A. Kazemzadeh, C. Busso, Z. Deng, S. Lee, and S. Narayanan, "An acoustic study of emotions expressed in speech," in Proc. of the 8th International Conference on Spoken Language Processing (ICSLP 04), Jeju Island, Korea, Oct. 2004, pp. 2193–2196.

[5] E. Moore, M. Clements, J. Peifer, and L. Weisser, "Critical analysis of the impact of glottal features in the classification of clinical depression in speech," IEEE Transactions on Biomedical Engineering, vol. 55, no. 1, pp. 96–107, Jan. 2008.

[6] C. Busso, S. Lee, and S. Narayanan, "Analysis of emotionally salient aspects of fundamental frequency for emotion detection," IEEE Transactions on Audio, Speech and Language Processing, vol. 17, no. 4, pp. 582–596, May 2009.

[7] J. Sundberg, S. Patel, E. Bjorkner, and K. R. Scherer, "Interdependencies among voice source parameters in emotional speech," IEEE Transactions on Affective Computing, vol. 2, no. 3, pp. 162–174, Jul. 2011.

[8] T. F. Yap, "Production under cognitive load: Effects and classification," Dissertation, The University of New South Wales, Sydney, Australia, 2012.

[9] P. N. Juslin and P. Laukka, "Communication of emotions in vocal expression and music performance: Different channels, same code?" Psychological Bulletin, vol. 129, no. 5, pp. 770–814, Sep. 2003.

[10] S. Patel and K. R. Scherer, Vocal behaviour. Berlin: Mouton-DeGruyter, 2013, pp. 167–204.

[11] P. Boersma, "Praat, a system for doing phonetics by computer," Glot International, vol. 5, no. 9/10, pp. 341–345, 2001.

[12] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer, F. Ringeval, M. Chetouani et al., "The INTERSPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism," in Proc. of INTERSPEECH. Lyon, France: ISCA, 2013, pp. 148–152.

[13] M. Schröder, Speech and Emotion Research. An Overview of Research Frameworks and a Dimensional Approach to Emotional Speech Synthesis, ser. Reports in Phonetics, University of the Saarland. Institute for Phonetics, University of Saarbrucken, 2004, vol. 7.

[14] M. Schröder, F. Burkhardt, and S. Krstulovic, "Synthesis of emotional speech," in Blueprint for Affective Computing, K. R. Scherer, T. Bänziger, and E. Roesch, Eds. Oxford: Oxford University Press, 2010, pp. 222–231.

[15] K. R. Scherer, "Vocal communication of emotion: A review of research paradigms," Speech Communication, vol. 40, pp. 227–256, 2003.

[16] K. Hammerschmidt and U. Jürgens, "Acoustical correlates of affective prosody," Journal of Voice, vol. 21, pp. 531–540, 2007.

[17] M. Goudbeek and K. R. Scherer, "Beyond arousal: Valence and potency/control cues in the vocal expression of emotion," Journal of the Acoustical Society of America (JASA), vol. 128, pp. 1322–1336, 2010.

[18] D. A. Sauter, F. Eisner, A. J. Calder, and S. K. Scott, "Perceptual cues in nonverbal vocal expressions of emotion," Quarterly Journal of Experimental Psychology, vol. 63, pp. 2251–2272, 2010.

[19] P. Laukka and H. A. Elfenbein, "Emotion appraisal dimensions can be inferred from vocal expressions," Social Psychological and Personality Science, vol. 3, pp. 529–536, 2012.

[20] B. Schuller, B. Vlasenko, F. Eyben, M. Wollmer, A. Stuhlsatz, A. Wendemuth, and G. Rigoll, "Cross-corpus acoustic emotion recognition: Variances and strategies," IEEE Transactions on Affective Computing (TAC), vol. 1, no. 2, 2010.

[21] A. Batliner, S. Steidl, B. Schuller, D. Seppi, K. Laskowski, T. Vogt, L. Devillers, L. Vidrascu, N. Amir, L. Kessous, and V. Aharonson, "Combining Efforts for Improving Automatic Classification of Emotional User States," in Proceedings 5th Slovenian and 1st International Language Technologies Conference, ISLTC 2006. Ljubljana, Slovenia: Slovenian Language Technologies Society, October 2006, pp. 240–245.

[22] D. Bone, C.-C. Lee, and S. Narayanan, "Robust unsupervised arousal rating: a rule-based framework with knowledge-inspired vocal features," IEEE Transactions on Affective Computing, vol. 5, no. 2, pp. 201–213, April 2014.

[23] B. Schuller, A. Batliner, D. Seppi, S. Steidl, T. Vogt, J. Wagner, L. Devillers, L. Vidrascu, N. Amir, L. Kessous, and V. Aharonson, "The Relevance of Feature Type for the Automatic Classification of Emotional User States: Low Level Descriptors and Functionals," in Proceedings INTERSPEECH 2007, 8th Annual Conference of the International Speech Communication Association, ISCA. Antwerp, Belgium: ISCA, August 2007, pp. 2253–2256.

[24] F. Weninger, F. Eyben, B. W. Schuller, M. Mortillaro, and K. R. Scherer, "On the Acoustics of Emotion in Audio: What Speech, Music and Sound have in Common," Frontiers in Emotion Science, vol. 4, no. Article ID 292, pp. 1–12, May 2013.

[25] F. Eyben, F. Weninger, and B. Schuller, "Affect recognition in real-life acoustic conditions – A new perspective on feature selection," in Proc. of INTERSPEECH 2013. Lyon, France: ISCA, Aug. 2013, pp. 2044–2048.

[26] M. Tahon and L. Devillers, "Acoustic measures characterizing anger across corpora collected in artificial or natural context," in Proc. of Speech Prosody, Chicago, IL, USA, 2010.

[27] T. F. Yap, J. Epps, E. Ambikairajah, and E. H. C. Choi, "Formant frequencies under cognitive load: Effects and classification," EURASIP Journal on Advances in Signal Processing, vol. 2011, pp. 1:1–1:11, Jan. 2011.

[28] A. C. Trevino, T. F. Quatieri, and N. Malyska, "Phonologically-based biomarkers for major depressive disorder," EURASIP Journal on Advances in Signal Processing, pp. 1–18, 2011.

[29] L. Tamarit, M. Goudbeek, and K. R. Scherer, "Spectral slope measurements in emotionally expressive speech," in Proc. of SPKD-2008. ISCA, 2008, paper 007.

[30] P. Le, E. Ambikairajah, J. Epps, V. Sethu, and E. H. C. Choi, "Investigation of spectral centroid features for cognitive load classification," Speech Communication, vol. 54, no. 4, pp. 540–551, 2011.

[31] N. Cummins, J. Epps, M. Breakspear, and R. Goecke, "An investigation of depressed speech detection: Features and normalization," in Proc. of Interspeech 2011, Florence, Italy, 2011, pp. 2997–3000.

[32] D. Ververidis and C. Kotropoulos, "Emotional speech recognition: Resources, features, and methods," Speech Communication, vol. 48, no. 9, pp. 1162–1181, Sep. 2006.

[33] B. Schuller, M. Wimmer, L. Mosenlechner, C. Kern, D. Arsić, and G. Rigoll, "Brute-Forcing Hierarchical Functionals for Paralinguistics: a Waste of Feature Space?" in Proceedings 33rd IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2008, IEEE. Las Vegas, NV: IEEE, April 2008, pp. 4501–4504.

[34] F. Eyben, A. Batliner, and B. Schuller, "Towards a standard set of acoustic features for the processing of emotion in speech," Proceedings of Meetings on Acoustics (POMA), vol. 9, no. 1, pp. 1–12, Jul. 2012.

[35] T. F. Yap, J. Epps, E. Ambikairajah, and E. H. C. Choi, "Voice source features for cognitive load classification," in Proc. of ICASSP 2011, Prague, Czech Republic. IEEE, 2011, pp. 5700–5703.

[36] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers, C. Müller, and S. Narayanan, "The INTERSPEECH 2010 Paralinguistic Challenge," in Proceedings INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, ISCA. Makuhari, Japan: ISCA, September 2010, pp. 2794–2797.

[37] B. Schuller, S. Steidl, A. Batliner, J. Epps, F. Eyben, F. Ringeval, E. Marchi, and Y. Zhang, "The interspeech 2014 computational paralinguistics challenge: Cognitive and physical load," in Proc. of INTERSPEECH 2014, Singapore. ISCA, 2014, to appear.

[38] J. R. Williamson, T. F. Quatieri, B. S. Helfer, R. Horwitz, B. Yu, and D. D. Mehta, "Vocal biomarkers of depression based on motor incoordination," in Proc. 3rd ACM Int. Workshop on Audio/Visual Emotion Challenge. ACM, 2013.

[39] V. Sethu, E. Ambikairajah, and J. Epps, "On the use of speech parameter contours for emotion recognition," EURASIP Journal on Audio, Speech and Music Processing (JASMP), 2013.

[40] B. Schuller and G. Rigoll, "Recognising Interest in Conversational Speech – Comparing Bag of Frames and Suprasegmental Features," in Proceedings INTERSPEECH 2009, 10th Annual Conference of the International Speech Communication Association, ISCA. Brighton, UK: ISCA, September 2009, pp. 1999–2002.

[41] E. Marchi, A. Batliner, B. Schuller, S. Fridenzon, S. Tal, and O. Golan, "Speech, Emotion, Age, Language, Task, and Typicality: Trying to Disentangle Performance and Feature Relevance," in Proceedings First International Workshop on Wide Spectrum Social Signal Processing (WS3P 2012), held in conjunction with the ASE/IEEE International Conference on Social Computing (SocialCom 2012), ASE/IEEE. Amsterdam, The Netherlands: IEEE, September 2012.

[42] S. Steidl, Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech. Berlin: Logos Verlag, 2009.

[43] B. Schuller, S. Steidl, A. Batliner, and F. Jurcicek, "The INTERSPEECH 2009 Emotion Challenge," in Proc. of INTERSPEECH, Brighton, UK, Sep. 2009, pp. 312–315.

[44] B. Schuller, A. Batliner, S. Steidl, F. Schiel, and J. Krajewski, "The INTERSPEECH 2011 Speaker State Challenge," in Proc. of INTERSPEECH, Florence, Italy, Aug. 2011, pp. 3201–3204.

[45] B. Schuller, S. Steidl, A. Batliner, E. Noth, A. Vinciarelli, F. Burkhardt, R. van Son, F. Weninger, F. Eyben, T. Bocklet, G. Mohammadi, and B. Weiss, "The INTERSPEECH 2012 Speaker Trait Challenge," in Proceedings INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association, ISCA. Portland, OR: ISCA, September 2012.

[46] B. Schuller, R. Müller, F. Eyben, J. Gast, B. Hörnler, M. Wöllmer, G. Rigoll, A. Hothker, and H. Konosu, "Being Bored? Recognising Natural Interest by Extensive Audiovisual Integration for Real-Life Application," Image and Vision Computing, Special Issue on Visual and Multimodal Analysis of Human Spontaneous Behavior, vol. 27, no. 12, pp. 1760–1774, November 2009.

[47] B. Schuller, B. Vlasenko, F. Eyben, G. Rigoll, and A. Wendemuth, "Acoustic emotion recognition: A benchmark comparison of performances," in Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU) 2009, Merano, Italy. IEEE, Nov. 2009, pp. 552–557.

[48] F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and B. Weiss, "A database of german emotional speech," in Proceedings Interspeech 2005, Lissabon, Portugal, 2005, pp. 1517–1520.

[49] T. Bänziger, M. Mortillaro, and K. R. Scherer, "Introducing the Geneva Multimodal Expression corpus for experimental research on emotion perception," Emotion, vol. 12, no. 5, pp. 1161–1179, 2012.

[50] K. R. Scherer, J. Sundberg, L. Tamarit, and G. L. Salomão, "Comparing the acoustic expression of emotion in the speaking and the singing voice," Computer Speech and Language, vol. 01, 2013.

[51] M. Grimm, K. Kroschel, and S. Narayanan, "The Vera am Mittag German Audio-Visual Emotional Speech Database," in Proc. of the IEEE International Conference on Multimedia and Expo (ICME), Hannover, Germany, 2008, pp. 865–868.

[52] M. Grimm, E. Mower, K. Kroschel, and S. Narayanan, "Primitives based estimation and evaluation of emotions in speech," Speech Communication, vol. 49, pp. 787–800, 2007.

[53] H. Schlosberg, "Three dimensions of emotion," Psychology Review, vol. 61, pp. 81–88, 1954.

[54] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten, "The WEKA data mining software: an update," ACM SIGKDD Explorations Newsletter, vol. 11, no. 1, pp. 10–18, 2009.

[55] V. Sethu, E. Ambikairajah, and J. Epps, "Speaker normalization of speech-based emotion recognition," in Proc. of the IEEE International Conference on Digital Signal Processing, Cardiff, UK, 2007, pp. 611–614.

[56] I. Fonagy, "La vive voix," Payo, 1983.

[57] M. Airas, H. Pulakka, T. Backstrom, and P. Alku, "A toolkit for voice inverse filtering and parametrisation," in Proc. of Interspeech 2005. ISCA, 2005, pp. 2145–2148.

[58] S. Patel, K. R. Scherer, J. Sundberg, and E. Björkner, "Mapping emotions into acoustic space: The role of voice production," Biological Psychology, vol. 87, pp. 93–98, 2011.

[59] F. Eyben, F. Weninger, F. Gross, and B. Schuller, "Recent developments in openSMILE, the munich open-source multimedia feature extractor," in Proc. of ACM MM 2013, Barcelona, Spain. New York, NY, USA: ACM, 2013, pp. 835–838.

[60] D. J. Hermes, "Measurement of pitch by subharmonic summation," Journal of the Acoustical Society of America, vol. 83, no. 1, pp. 257–264, 1988.

[61] B. Schuller, Intelligent Audio Analysis, ser. Signals and Communication Technology. Springer, 2013.

[62] E. Zwicker and H. Fastl, Psychoacoustics – Facts and Models. Springer, 1999.

[63] H. Hermansky, "Perceptual linear predictive (plp) analysis for speech," Journal of the Acoustical Society of America, vol. 87, pp. 1738–1752, 1990.

[64] J. Makhoul and L. Cosell, "Lpcw: An lpc vocoder with linear predictive spectral warping," in Proc. of ICASSP'76, Philadelphia, USA. IEEE, 1976, pp. 466–469.

[65] B. Hammarberg, B. Fritzell, J. Gauffin, J. Sundberg, and L. Wedin, "Perceptual and acoustic correlates of abnormal voice qualities," Acta Otolaryngologica, vol. 90, pp. 441–451, 1980.

[66] S. Patel, K. R. Scherer, J. Sundberg, and E. Björkner, "Acoustic markers of emotions based on voice physiology," in Proc. of Speech Prosody 2010, Chicago, IL, USA, 2010.

[67] J. Makhoul, "Linear prediction: A tutorial review," Proceedings of the IEEE, vol. 63, no. 5, pp. 561–580, Apr. 1975.

[68] S. Young, G. Evermann, M. Gales, T. Hain, D. , X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland, The HTK Book for HTK Version 3.4. Cambridge University Engineering Department, 2006.

## Key Citations for Follow-up

**[17] Goudbeek & Scherer (2010) - Beyond arousal**: Foundational work on vocal emotion dimensions (arousal and valence) that directly informed GeMAPS parameter selection. Essential for understanding why certain acoustic features correlate with emotional states.

**[23] Schuller et al. (2007) - The Relevance of Feature Type**: Early work demonstrating which low-level descriptors vs. functionals matter most for emotion classification, directly guiding the minimalistic feature set design.

**[29] Tamarit et al. (2008) - Spectral slope measurements**: Validates spectral slope parameters as emotionally significant, supporting inclusion in GeMAPS.

**[59] Eyben et al. (2013) - openSMILE**: The reference implementation toolkit that provides the actual extraction algorithms for all GeMAPS features, essential for reproducibility and practical implementation.

**[62] Zwicker & Fastl (1999) - Psychoacoustics**: Foundational psychoacoustics reference explaining human loudness perception, underlying the auditory weighting and MFCC components of GeMAPS.

