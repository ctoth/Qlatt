# Pages 160-199 Notes

## Chapters/Sections Covered
- **Chapter 5 (continued):** Training Data and Procedures
  - 5.1.1 Lexical-postlexical alignment (continued from previous chunk)
  - 5.1.2 Creation of postlexical training database
  - 5.2 Characterization of learning problem
  - 5.3 Neural network architecture
  - 5.4 Data encoding
    - 5.4.1 Features for lexical phones
    - 5.4.2 Stress
    - 5.4.3 Syntactic and prosodic information
    - 5.4.4 Windowing
- **Chapter 6:** Results
  - 6.1 Analysis of neural network
  - 6.2 General phonological analysis
  - 6.3 General error analysis
  - 6.4 Allophony
    - 6.4.1 Vowel fronting

## Key Findings

### Lexical-Postlexical Alignment
- Pseudophones used to collapse postlexical phones for alignment (stop closures/releases, glottal stops, vowels)
- Cost table for alignment: insertions and deletions cost 7, substitutions not otherwise covered cost 10 (following Young et al. 1995)
- Dynamic programming approach developed by iteratively listing probable lexical-postlexical correspondences

### Neural Network Performance
- **87.8%** of predicted phones matched original hand-labeled files
- **98%** total acceptable phones when including:
  - Another allophone of original phone: 8.6%
  - Another allomorph of original phone: 1.4%
  - Another dialect's version of original phone: 0.002%
- Only **2%** unacceptable errors

### Window Size Experiments
- Tested windows of length 3, 5, and 9
- Results:
  | Window size | % matching vectors |
  |-------------|-------------------|
  | 3           | 61.2              |
  | 5           | 92.9              |
  | 9           | 94.9              |
- Window of 9 (4 phones each direction) performed best
- Longer windows may capture long-distance dependencies and prosodic effects

### Weight Distribution Analysis
- Current phone (position 5 in window) has most influence on output
- Following phone has more influence than preceding phone (consistent with anticipatory assimilation being more productive than perseverative)
- Lexical phones and features contribute more than stress and boundary information

### Postlexical Phenomena Learned
The neural network successfully learned:
- Unreleased stops (fed -> fed')
- Glottalized vowels (and -> ?aend)
- Glottalized consonants (straight -> stre?)
- /d/ deletion (and follow -> aen falo)
- /t/ deletion (abrupt start -> abr^p' start')
- Destressing and assimilation
- /t/ flapping (dirty -> da'ri)
- Nasal flapping (corner -> kor~a')
- /h/ voicing (in her -> in fia')
- Schwa epenthesis (curls -> ka'lz)
- /u/ fronting (dune -> dun)
- Syllabic consonants (poodles -> pudlz)

### Dialect/Labeling Idiosyncrasies Learned
- marry/merry/Mary merger
- Schwa deletion/metathesis
- Laxing before /r/
- Wh voicing

## Equations Found

### Equation 5-1: Entropy
$$H(W) = -\sum_{w \in V(W)} P(w) \log_2 P(w)$$

Where:
- $H(W)$ = entropy for lexical phone W
- $V(W)$ = set of possible postlexical reflexes of W
- $P(w)$ = probability of each reflex occurring

**Purpose:** Measures uncertainty in predicting postlexical output given a lexical phone. Higher entropy = more variable/harder to predict.

## Parameters/Rules Found

| Rule/Parameter | Description | Value/Pattern | Page |
|----------------|-------------|---------------|------|
| Insertion cost | DP alignment cost for insertions | 7 | 147 |
| Deletion cost | DP alignment cost for deletions | 7 | 147 |
| Substitution cost | Default substitution cost | 10 | 147 |
| Output phones | Number of possible postlexical outputs | 117 | 168 |
| Input block 2 | Lexical phone labels (1-of-n encoding) | 46*9=414 | 160 |
| Input block 3 | Lexical phone features | 53*9=477 | 160 |
| Input block 4 | Stress/prominence information | 4*9=36 | 160 |
| Input block 5 | Boundary information | 14*9=126 | 160 |
| Hidden layer (blocks 6-9) | First hidden layer | 10 units each | 160 |
| Hidden layer (block 10) | Second hidden layer | 20 units | 160 |
| Hidden layer (block 11) | Third hidden layer | 117 units | 160 |
| Stress encoding | Primary/secondary/unstressed | 1 / 0.5 / 0 | 165 |
| Word prominence | O'Shaughnessy (1976) scale | 0-14 | 165 |
| Word type | Function vs content word | 0 / 1 | 165 |

## Algorithms/Procedures

### Lexical-Postlexical Alignment Algorithm
1. Create pseudophones by collapsing related postlexical phones (e.g., stop closures + releases)
2. Build cost table for substitutions based on phonological similarity
3. Use dynamic programming with costs: insertion=7, deletion=7, substitution=10 (default)
4. Align lexical and postlexical strings to create training pairs
5. Store in relational database with contextual information

### Neural Network Training Procedure
1. Encode lexical phone as 1-of-n in Block 2
2. Encode phonological features in Block 3 (46 features per phone)
3. Encode stress/prominence in Block 4 (4 values: stress, prominence, word type, pitch accent)
4. Encode boundary information in Block 5 (14 binary features)
5. Window all inputs with size 9 (4 phones left, current, 4 phones right)
6. Pad sentence boundaries with blank phones
7. Pass through hidden layers (blocks 6-11)
8. Output 1 of 117 postlexical phones

## Figures of Interest

- **Figure 5-1 (page 158):** Entropy of lexical phones - bar chart showing /t/ and schwa have highest entropy (~2.5+ bits), while /s/, /m/, /w/, /j/ have lowest (~0 bits). High entropy phones are hardest to predict.

- **Figure 5-2 (page 158):** Number of postlexical reflexes per lexical phone - schwa has most (~22), followed by /t/, /d/, /k/. Not perfectly correlated with entropy since distribution matters.

- **Figure 5-1 (page 160):** Postlexical neural network architecture diagram showing:
  - Block 1: OUTPUT (117 units)
  - Block 11: Hidden layer (connected to output)
  - Block 10: Hidden layer (20 units)
  - Blocks 6-9: Hidden layers (10 units each)
  - Blocks 2-5: INPUT streams

- **Figure 6-1 (page 170):** TDNN window weights for phone label stream - shows current phone (position 5) has weight ~2500, decreasing to ~500 for positions 1,2,8,9. Demonstrates current phone dominates prediction.

- **Figure 6-2 (page 171):** Weight distribution by input type - lexical features (~800) > lexical phones (~750) > boundaries (~350) > stress (~250). Shows features and phones more important than prosodic info.

- **Figure 6-1 (page 183):** Distribution of schwa allophones by coronal environment - shows neural network achieves 67-83% accuracy in predicting [i] vs [ax] based on surrounding coronals.

## Tables of Interest

- **Table 5-6 (page 146):** Postlexical pseudophones - lists 40 composite symbols used for alignment (glottal stops + vowels, stop closures + releases, affricates)

- **Table 5-7 (page 146):** Aligned lexical and postlexical phones with pseudophones example

- **Table 5-9 (page 149):** Complete cost table for lexical-postlexical alignment - 90+ phone pairs with cost 0 (acceptable substitutions)

- **Table 5-1 (page 152):** Fields in lexical-postlexical database (24 fields including orthographic word, phones, stress, boundaries, etc.)

- **Table 5-1 (pages 155-157):** Postlexical reflexes of each lexical phone - comprehensive frequency counts showing:
  - /a/: 405 instances as [a], 17 as [?a], 7 deleted
  - /t/: 566 as [tc th], 273 as [tc], 214 as [tc t], 170 as [?], 134 as [th], etc.
  - Schwa /ax/: 622 deleted, 453 as [i], 354 as [^], 337 as [ax], etc.

- **Table 5-1 (pages 163-164):** Features for lexical phones - 53 binary features including vocalic, vowel, sonorant, obstruent, continuant, affricate, nasal, approximant, place features (front, mid, back, high, low), manner features (bilabial through uvular), voiced, round, etc.

- **Table 5-1 (page 165):** Neural network block 4 input (stress features):
  - Syllable stress (1=primary, 0.5=secondary, 0=unstressed)
  - Word prominence (0-14, O'Shaughnessy 1976)
  - Word type (0=function, 1=content)
  - Pitch accent (binary)

- **Table 5-1 (page 166):** Neural network block 5 input (boundary features) - 14 binary features for prosodic/syntactic boundaries (syllable, word, phrase, clause, sentence, intermediate phrase, intonation phrase)

- **Table 5-1 (page 168):** Window size performance comparison

- **Table 6-1 (page 172):** Postlexical phenomena learned by network - 13 phenomena with lexical/postlexical examples

- **Table 6-2 (page 173):** Dialect/labeling idiosyncrasies learned

- **Table 6-1 (page 175):** Summary of postlexical network results (the key performance table)

- **Table 6-2 (page 176):** Allophonic errors - detailed breakdown of 225 cases where network chose different allophone

- **Table 6-3 (page 178):** Destressing errors - 37 cases of vowel reduction mismatches

- **Table 6-4 (page 179):** Dialect errors - 7 cases of dialect variant mismatches

## Quotes Worth Preserving

> "In a faithful TTS implementation of the glottalization rule, the phonological structure will have to be parsed up to the intonation phrase, and the rule will have to have access to both the immediate segmental context and the prosodic strength of the surrounding syllables." (p. 167, quoting Pierrehumbert and Frisch 1997)

> "it is better to focus on issues such as representations and prior knowledge rather than the learning dynamics, since there are no general purpose learning algorithms which are good for an arbitrary prior." (p. 160, quoting Cohen 1997)

> "Browman and Goldstein (1992) discuss the concept of a 'targetless' schwa. Such a schwa takes on the characteristics of surrounding phonetic material, in contrast with views holding that schwa's target is in the center of the vowel space, with reduced vowels shifting towards it." (p. 184)

## Implementation Notes

### For G2P/TTS Systems
1. **Phonological feature encoding is critical** - the network uses 53 features per phone including place, manner, voicing, vowel position, and distinctive features. More important than raw phone identity.

2. **Window size matters** - 9 phones (4 left, current, 4 right) captures most relevant context. Smaller windows dramatically reduce accuracy.

3. **Prosodic information helps but less than expected** - stress, prominence, and boundary features contribute less than phonological features to the prediction.

4. **Anticipatory effects dominate** - following context is more predictive than preceding context (asymmetric window weights).

5. **98% acceptable rate achievable** - with careful definition of "acceptable" (including allophones, allomorphs, dialect variants).

6. **High-entropy phones to watch:**
   - /t/ - many realizations (aspirated, unreleased, flap, glottal stop, deleted)
   - Schwa - frequently deleted or realized as various reduced vowels
   - /d/ - deletion, flapping, glottalization
   - /k/ - aspiration, unreleased, deletion

7. **Vowel fronting rule for /u/ and schwa:**
   - Coronal environment (on left AND right, or following coronal alone) favors fronted variants [u] and [i]
   - Non-coronal environment (on left AND right, or preceding non-coronal alone) favors unfronted variants [u] and [ax]
   - Intervocalic /l/ and syllable-final /l/ are NOT coronal for this purpose

8. **Database fields for postlexical prediction:**
   - Previous/next phone IDs for context
   - Syllable stress level
   - Word prominence (0-14)
   - Word type (function/content)
   - 14 boundary features (left/right for syllable, word, phrase, clause, sentence, intermediate phrase, intonation phrase)

### Vowel Fronting Analysis
- /u/ fronts to [u] before/after coronals (including /j/)
- Schwa /ax/ realized as [i] near coronals, [ax] elsewhere
- Syllable-final dark /l/ blocks fronting despite coronal component
- Neural network achieves 67-83% accuracy on schwa allophone prediction by environment

## Cross-References

### Key Citations
- **Young et al. (1995)** - Cost values for DP alignment (insertions=7, deletions=7, substitutions=10)
- **Riley and Ljolje (1996)** - Phonemic-phonetic mapping, feature-based alignment
- **Gildea and Jurafsky (1996)** - Feature-based alignment approach
- **Keating et al. (1994)** - Database structure for TIMIT analysis
- **O'Shaughnessy (1976)** - Word prominence scale (0-14)
- **Pierrehumbert (1980)** - Pitch accent notation (H*, L*)
- **Nespor and Vogel (1986)** - Prosodic hierarchy
- **Pierrehumbert and Frisch (1997)** - Glottalization rule requires parsed prosodic structure
- **Pierrehumbert and Beckman (1988)** - Running window on parsed phonological structures
- **Church (1986)** - Long-distance dependencies in letter-to-sound conversion
- **Sejnowski and Rosenberg (1987)** - NETtalk, window length 7
- **McCulloch et al. (1987)** - Window length 9
- **Cohen (1997)** - Focus on representations over learning dynamics
- **Karaali (1994)** - Monnet neural network simulator, sequential/random training modes
- **Giegerich (1992)** - Anticipatory assimilation more productive than perseverative in English
- **Hock (1986)** - Same observation across languages
- **Randolph (1989)** - Percent information extracted for stop realization
- **Breiman et al. (1993)** - CART (Classification and Regression Trees)
- **Sankoff (1988)** - VARBRUL for /t,d/ deletion prediction
- **Selkirk (1984)** - Destressing patterns in function words
- **Browman and Goldstein (1992)** - "Targetless" schwa concept
- **Van Bergem (1994)** - Schwa lacks articulatory target, determined by phonemic context
- **Sproat and Fujimura (1993)** - /l/ has vocalic dorsal + consonantal apical gesture
- **Hume and Clements (1992, 1995)** - Front vowels possess coronal feature
- **Clements (1976), Zsiga (1995)** - /j/ patterns with coronals
- **McLemore (1995)** - Predicting [I]-like reduced vowels
- **Byrd (1994)** - TIMIT database analysis, [i] vs [ax] distribution by gender and region
- **Jensen (1993)** - Vowel reduction rules, reduced vowels before palatals become [I]
