This paper investigates whether vocal emotion recognition is universal or culture-specific by examining how listeners from 9 countries (Europe, US, and Indonesia) recognize emotions encoded in semantically meaningless sentences. The study demonstrates robust cross-cultural emotion recognition with 66% mean accuracy across all emotions and countries, with highly correlated confusion patterns (r = .85) across cultures, showing that anger is most reliably recognized vocally while joy is frequently confused with neutral. These findings are critical for speech synthesis because they identify which emotions have clear universal acoustic markers that can be reliably synthesized and reproduced, while revealing acoustic confusions that must be deliberately addressed through careful parameter tuning to ensure synthesized emotional speech is perceived consistently across listeners regardless of cultural background.
