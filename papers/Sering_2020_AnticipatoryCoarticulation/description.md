This paper compares segment-based and gradient-based recurrent neural network approaches for producing anticipatory coarticulation in VocalTractLab, a 3D articulatory speech synthesizer. The key finding is that gradient-based planning can capture formant-level coarticulation effects but fails to reproduce the differential articulatory patterns (tongue raising) that humans use to distinguish upcoming vowel contexts. For Klatt synthesis, this demonstrates that formant targets must look ahead to upcoming vowels beyond simple phone-to-phone blending, suggesting that anticipatory coarticulation rules require long-range phoneme lookahead to achieve natural vowel-to-vowel transitions.
