# Abstract

## Original Text (Verbatim)

Professional actors' portrayals of 14 emotions varying in intensity and valence were presented to judges. The results on decoding replicate earlier findings on the ability of judges to infer vocally expressed emotions with much-better-than-chance accuracy, including consistently found differences in the recognizability of different emotions. A total of 224 portrayals were subjected to digital acoustic analysis to obtain profiles of vocal parameters for different emotions. The data suggest that vocal parameters not only index the degree of intensity typical for different emotions but also differentiate valence or quality aspects. The data are also used to test theoretical predictions on vocal patterning based on the component process model of emotion (K. R. Scherer, 1986). Although most hypotheses are supported, some need to be revised on the basis of the empirical evidence. Discriminant analysis and jackknifing show remarkably high hit rates and patterns of confusion that closely mirror those found for listener-judges.

---

## Our Interpretation

This paper investigates whether listeners can reliably identify emotions from voice alone, and what acoustic features distinguish different emotional states. The key finding is that 14 different emotions have measurable acoustic "profiles" based on parameters like F0 (pitch), energy, speech rate, and spectral characteristics - with recognition accuracy around 48-55%, well above the 7% chance level. For speech synthesis, this provides empirical acoustic parameter targets for generating emotionally expressive synthetic speech.
