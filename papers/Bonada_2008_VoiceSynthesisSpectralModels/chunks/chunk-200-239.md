# Pages 200-239 Notes

## Chapters/Sections Covered
- Section 3.5 Sound Rendering (continuation from p.200)
  - Phase Concatenation Using Harmonic Trajectories
  - Voice Model Residual Envelope Concatenation
- Section 3.6 Evaluation
  - 3.6.1 Test Design
  - 3.6.2 Results (Discussion)
- Section 3.7 Conclusions
- Chapter 4: Conclusions
  - Future Perspectives
  - Summary of Contributions
- Annex A: Vocaloid Commercial Software
- Annex B: Spanish Recording Scripts
- Annex C: Publications by the author
- Annex D: Patents by the author
- Annex E: Audio References

## Key Findings

### Phase Concatenation (pp.200-202)
- To avoid harmonic phase discontinuities at segment boundaries, phase continuity condition must be enforced
- Phase correction can be applied to left or right of boundary (reversed sign) and spread along several frames for smooth transition
- Period phase discontinuities occur at voice pulse onset sequences - requires time-shift correction
- Time-shift the right sample relative to left sample using MFPA onset estimation

### Residual Envelope Concatenation (pp.201-203)
- EpR (Excitation plus Resonances) model used to avoid spectral shape discontinuities
- Frequency warping function applied to transition frames at left of boundary
- Mapping uses factor `1-SSIntp` (0 at boundary, 1 at start of interpolation zone)
- Differential envelope (weighted by SSIntp) added to achieve smooth timbre transitions
- Figure 3.26 shows clear waveform discontinuity reduction with concatenation smoothing

### Listening Test Design (pp.204-207)
- 50 participants tested in studio at Universitat Pompeu Fabra
- Half master students (MTG), half external people
- 24 audio excerpts rated on 12 questions each (Q1-Q12)
- Evaluation criteria: expressiveness, naturalness, pleasantness, nasality, singer quality, tuning, pronunciation, lyrics understanding, song liking
- Rating scale: 1-5 for most questions
- Audio excerpts included: real singers, thesis synthesis (DS), Vocaloid (VS), other synthesizers (OS)
- Synthesizers compared: NTT, FLINGER, CANTOR, MERON, MUSSE, VocalWriter, CHANT, Vocaloid

### Listening Test Results (pp.208-218)
- **Q1 Expressiveness**: SP (singer performances) rated highest (~4), DS examples with manual controls rated ~3.5
- **Q2 Naturalness**: SP excerpts rated most natural (~4), DS "ansiedad male synth" slightly below 4
- **Q3 Pleasantness**: SP excerpts ~4, DS choir and synthesis examples ~3.8
- **Q4 Nasality**: SP excerpts least nasal (~2), most synthesis examples ~3
- **Q5 Singer quality**: SP rated as good singers, DS choir and "ansiedad male" best synthetic
- **Q6 Tuning**: OS CHANT rated best tuned (~4.7), SP jazz and ansiedad ~4
- **Q7 Pronunciation**: DS and SP child examples worst, CANTOR/FLINGER/VocalWriter worst synthetic
- **Q8 Lyrics understanding**: Language-dependent - Spanish well understood, Japanese not understood
- **Q12 Synthesized voice detection**: DS "ansiedad male synth" convinced ~34% it was real, 30% thought it was real singer

### Key Evaluation Findings (p.211)
- Thesis synthesis excerpts rated higher than other synthesizers in most categories
- DS examples sound in general more natural than OS examples
- Best generated example: "ansiedad male synth" - convinced 1/3 of subjects it was real
- Mozart's aria with CHANT considered most convincing synthesis until this work
- Real singer performances still best rated and distinguishable from synthetic

### Chapter 3 Conclusions (p.219)
- Performance sampling approach based on concatenative synthesis, not generic phoneme models
- Two main processes: (1) trajectory generation within sonic space, (2) trajectory rendering via sample concatenation
- Trajectory cost function considers phonetic timing distance, pitch, loudness, concatenation continuity
- Phase model essential for simplifying and improving sample concatenation

### Chapter 4 Conclusions (pp.220-224)
- Performance sampling models the sonic space of performer+instrument combination
- WBVPM (Wide-Band Voice Pulse Modeling) combines time-domain and frequency-domain advantages
- Amplitude spectral voice model distinguishes voice source from vocal tract
- Phase model predicts harmonic phase at voice pulse onsets without altering perceptual timbral characteristics
- Research incorporated into Yamaha's Vocaloid software
- Several international patents filed (see Annex D)

### Future Perspectives (p.223)
- Timbre model needs more realism - machine learning (SVR, NNs, GMMs) could help
- Expression modeling via training system on singer's performances using HMMs
- Create new virtual singers from just a few recorded songs
- Extend concepts to other instruments (violin synthesizer in progress)

## Equations Found

### Phase Continuity Condition (Eq. 3.5, p.200)
$$\phi'_{0,h,m} \approx \text{princarg}\left(\phi'_{0,h,m-1} + 2\pi \frac{f'_{h,m-1} + f'_{h,m}}{2} \Delta_t\right)$$

Where:
- $\phi'_{0,h,m}$ and $\phi'_{0,h,m-1}$ = phases of harmonic $h$ at right and left frames respectively
- $f'_{h,m}$ = frequency of harmonic $h$ at frame $m$
- $\Delta_t$ = time between frames

### Phase Correction (Eq. 3.6, p.200)
$$\Delta\phi_h^c = \text{princarg}\left(\phi'_{0,h,m} - 2\pi \frac{f'_{h,m-1} + f'_{h,m}}{2} \Delta_t - \phi'_{0,h,m-1}\right)$$

### Time-Shift Correction (Eq. 3.7, p.202)
$$\Delta_t^c = \frac{\text{princarg}\left(\phi'_{0,0,m-1} + 2\pi \frac{f'_{0,m-1} + f'_{0,m}}{2}\Delta_t - \phi'_{0,0,m}\right)}{2\pi f'_{0,m}} = \frac{-\Delta\phi_0^c}{2\pi f'_{0,m}}$$

### Phase Correction with Time-Shift (Eq. 3.8, p.202)
$$\Delta\phi_h^c = \text{princarg}\left(\phi'_{0,h,m} + 2\pi f'_{h,m}\Delta_t^c - 2\pi \frac{f'_{h,m-1} + f'_{h,m}}{2}\Delta_t - \phi'_{0,h,m-1}\right)$$

### Rating Statistics (Eq. 3.9, p.208)
$$r_{\min} = \min(r_i) \; \forall i \in [1,2,...,I] \qquad r_{\max} = \max(r_i) \; \forall i \in [1,2,...,I]$$

$$r_m = \frac{\sum_{i=1}^{I} r_i}{I}$$

$$\sigma_{ra} = \sqrt{\frac{\sum_{i=1}^{I}(r_m - r_i)^2 \cdot (r_m > r_i)}{\sum_{i=1}^{I}(r_m > r_i)}} \qquad \sigma_{rb} = \sqrt{\frac{\sum_{i=1}^{I}(r_m - r_i)^2 \cdot (r_m < r_i)}{\sum_{i=1}^{I}(r_m < r_i)}}$$

Where:
- $I$ = number of subjects
- $r_i$ = rating by subject $i$
- $r_m$ = mean rating
- $\sigma_{ra}$ = standard deviation above mean
- $\sigma_{rb}$ = standard deviation below mean

## Parameters Found

| Name | Symbol | Units | Value/Range | Notes |
|------|--------|-------|-------------|-------|
| Spectral Shape Interpolation Factor | SSIntp | - | 0-1 | 0 at interpolation start, 1 at boundary |
| Pitch transposition factor | $T_{pitch}$ | ratio | 0.65-1.75 | For audio transformations |
| Timbre scaling factor | $T_{timbre}$ | ratio | 0.95-1.18 | For formant-pitch modification |
| F0 range (female singing) | $f_0^{range}$ | Hz | 350-386 | Example from test |
| F0 range (female scat fast) | $f_0^{range}$ | Hz | 127-318 | Example from test |
| F0 range (male soul) | $f_0^{range}$ | Hz | 98-211 | Example from test |
| F0 range (male speech) | $f_0^{range}$ | Hz | 59-100 | Example from test |
| F0 range (female speech) | $f_0^{range}$ | Hz | 190-410 | Example from test |
| F0 range (male breathy) | $f_0^{range}$ | Hz | 270-396 | Example from test |
| F0 range (male gospel) | $f_0^{range}$ | Hz | 145-357 | Example from test |

## Rules/Algorithms

### Phase Concatenation Algorithm (p.200-202)
1. For each harmonic h at boundary:
   a. Compute ideal phase continuation using linear frequency interpolation
   b. Calculate phase correction as difference from actual right-frame phase
   c. Optionally time-shift right sample using fundamental phase difference
2. Spread correction across several frames for smooth transition
3. If applied to both sides, spread half correction to each side

### Residual Envelope Concatenation (p.202-203)
1. Estimate EpR of boundary frames using resonance frequencies (F0, F1, F2, F3...)
2. Stretch left EpR using frequency warping from left resonances to right resonances
3. Compute differential envelope between right EpR and stretched left EpR
4. For each transition frame at left of boundary:
   a. Compute frequency warping function with factor (1-SSIntp)
   b. Apply warping to frame's spectral amplitude envelope
   c. Add differential envelope weighted by SSIntp

### Listening Test Protocol (pp.204-206)
1. Install audio equipment in recording studio (2 speakers, 12 chairs)
2. Present 24 audio excerpts with 12 questions each
3. Each excerpt repeated twice, ~1.5 min per excerpt
4. First two excerpts discarded from evaluation (familiarization)
5. Order randomized across sessions to reduce order effects
6. Collect demographic data: age, country, musical education, listening habits

## Figures of Interest
- **Fig 3.25 (p.203):** Timbre concatenation using EpR - shows frequency mapping and differential envelope process
- **Fig 3.26 (p.203):** Waveform comparison (a) without smoothing showing discontinuities vs (b) with smoothing showing minimized discontinuities
- **Fig 3.27 (p.208):** Rating statistics visualization showing min/max boxes and mean deviation
- **Fig 3.28-3.29 (pp.212-213):** User profile statistics - age distribution, country, musical education
- **Fig 3.30-3.33 (pp.214-217):** Audio ratings for all 12 questions - comparing original, thesis synthesis, vocaloid, and other synthesizers
- **Fig 3.34 (p.218):** Lyrics understanding vs language level correlation

## Quotes Worth Preserving

> "In general, results are judged to be of high quality. However, when processing by spectral regions further work is needed to tackle phonation modes and avoid audible discontinuities such as those found in breathy to non-breathy connections." (p.200)

> "The best generated example is with no doubt ansiedad male synth, which convinced one third of the test subjects to be a real performance, while almost another third doubted." (p.211)

> "We believe this synthesis result is much more challenging that the well-known Mozart's aria example synthesized with CHANT, probably the best singing voice synthetic example till now." (p.211)

> "Although sampling has been considered a way to capture and reproduce the sound of an instrument, it should be better considered a way to model the sonic space produced by a performer with an instrument." (p.220)

> "A significant part of our research results in singing voice synthesis have been incorporated to the Yamaha's virtual singer software Vocaloid." (p.222)

## Implementation Notes

### Phase Concatenation
- Use `princarg()` function to wrap phase to [-pi, pi]
- Time-shift computed from fundamental phase discrepancy
- Higher harmonics get larger phase corrections (scaled by harmonic number in frequency term)
- Consider streaming/real-time: time-shift right sample, not left (affects all previous samples)

### EpR Interpolation
- Frequency warping similar to vocal tract length normalization
- SSIntp factor controls interpolation weight (0 = no warping, 1 = full target)
- Differential envelope captures spectral shape differences between samples

### Evaluation Metrics
- Asymmetric standard deviations (above/below mean) capture rating distribution shape
- Color coding: gray=real, blue=thesis synthesis, orange=vocaloid, green=other synthesizers

### Audio Reference Database (Annex E)
- Available at http://www.mtg.upf.edu/~jbonada/thesis
- Includes voice source, harmonics as spectral regions, shape variance, WBVPM, phase model examples
- Transformation parameters documented: $T_{pitch}$, $T_{timbre}$, strategy numbers

### Spanish Recording Scripts (Annex B)
- SUSTAINS: 7 pitches x 4 dynamics (very soft, soft, normal, loud)
- ARTICULATIONS: 3 pitches (normal, +5 semitones, -4 semitones) x 2 tempos x 2 dynamics
- 116 sentences designed for phonetic coverage

### Vocaloid Integration (Annex A)
- MTG research incorporated March 2003
- Voice processing in frequency domain
- Vocal libraries from actual singers
- Expressive effects: vibrato, pitch bends
- Languages: Japanese, English (initially)
